{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ececute all cells sequntially on everyday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules for bert\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from bert.tokenization import FullTokenizer     # Still from bert module\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "#     physical_devices = tf.config.list_physical_devices('GPU') \n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "max_seq_length = 280\n",
    "    \n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,),dtype=tf.int32,name=\"input_word_ids\")\n",
    "    \n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,),dtype=tf.int32,name=\"input_mask\")\n",
    "    \n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,),dtype=tf.int32,name=\"segment_ids\")\n",
    "    \n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",trainable=True)\n",
    "    \n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    \n",
    "    #made the bert model\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
    "\n",
    "tf.gfile = tf.io.gfile\n",
    "    \n",
    "#geting vocab file and tokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "#function to produce embeddings\n",
    "def get_bert_pool(input_ids,input_masks,input_segments):\n",
    "    pool_embs,all_embs=model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    return(pool_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract bert embedings on a given batch of data\n",
    "#batch is used to avoid problems due to GPU RAM\n",
    "def extract_bert_embed(df):\n",
    "            \n",
    "    #tokenizing tweets\n",
    "    tokens=df.text.apply(lambda text: tokenizer.tokenize(text))\n",
    "    \n",
    "    #input_ids\n",
    "    input_ids=tokens.apply(lambda row:get_ids(row,tokenizer,max_seq_length))\n",
    "    #input_masks\n",
    "    input_masks=tokens.apply(lambda row:get_masks(row,max_seq_length))\n",
    "    #input_segments\n",
    "    input_segments=tokens.apply(lambda row:get_segments(row,max_seq_length))\n",
    "    \n",
    "    #turning each list in row into numpy array\n",
    "    input_ids=input_ids.apply(lambda x:np.array(x))\n",
    "    input_masks=input_masks.apply(lambda x:np.array(x))\n",
    "    input_segments=input_segments.apply(lambda x:np.array(x))\n",
    "    \n",
    "    #now turning them into dataframes\n",
    "    input_ids=pd.DataFrame(input_ids)\n",
    "    input_masks=pd.DataFrame(input_masks)\n",
    "    input_segments=pd.DataFrame(input_segments)\n",
    "    \n",
    "    #naming coloumns of each one from 'text'->respective dataframe name\n",
    "    input_ids.columns=['input_ids']\n",
    "    input_masks.columns=['input_masks']\n",
    "    input_segments.columns=['input_segments']\n",
    "    \n",
    "    #now joining theminto one dataframe\n",
    "    input=input_ids.join([input_masks,input_segments])\n",
    "    \n",
    "    #now extracting bert embeddings\n",
    "    pool=input.apply(lambda tweet:get_bert_pool(tweet.input_ids,tweet.input_masks,tweet.input_segments),axis=1)\n",
    "    \n",
    "    #dealing with rows with sublist of type[[0,1,...767]]->[0,1,...,767] and changing to dataframe\n",
    "    pool=pool.apply(lambda x:x[0])\n",
    "    pool=pd.DataFrame(pool)\n",
    "    \n",
    "    #now changing each list in the row to dataframe of 768 columns\n",
    "    pool=pd.DataFrame(pool.to_dict()[0]).T\n",
    "    \n",
    "    return(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if tf is using GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "today=dt.date.today()\n",
    "\n",
    "#file paths\n",
    "embed_path='F:/twitter_data/twitter_users_data/bert_emb/'\n",
    "data_path='F:/twitter_data/twitter_users_data/'\n",
    "tweets_file='tweet_{}.csv'.format(str(today))\n",
    "embed_file='bert_embed_{}.csv'.format(str(today))\n",
    "\n",
    "#df is the tweets gathered today\n",
    "df=pd.read_csv(data_path+tweets_file)\n",
    "#input_df is the tweet text\n",
    "input_df=pd.DataFrame(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract bert embeddings in batchs \n",
    "def batchwise_bert_embed_extractor(input_df,batch_size=200):\n",
    "    start=0\n",
    "    #empty list for pools\n",
    "    pool=[]\n",
    "    \n",
    "    #working on batch of tweets\n",
    "    while(start<=len(input_df)):\n",
    "        #input batch for bert embedings extraction\n",
    "        pool_input_df=pd.DataFrame(input_df.iloc[start:start+batch_size])\n",
    "        #extracted bert embedings on the batch\n",
    "        pool_df=extract_bert_embed(pool_input_df)\n",
    "        #adding embedings dataframe to pool\n",
    "        pool.append(pool_df)\n",
    "        #new batch\n",
    "        start=start+batch_size\n",
    "    \n",
    "    #making pool_frame from list of pools\n",
    "    pool_frame=pd.concat(pool,axis=0,ignore_index=True)\n",
    "    #exporting pool_frame to file location as csv\n",
    "    pool_frame.to_csv(embed_path+embed_file,index=False)\n",
    "\n",
    "#this will execute on todays tweets\n",
    "batchwise_bert_embed_extractor(input_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
